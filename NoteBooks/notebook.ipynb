{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Access the api keys\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\RAGProjects\\\\RAG-Questions-Creator\\\\NoteBooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the current dire\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the data from the data directory\n",
    "def data_extraction(data_dir):\n",
    "     loader = DirectoryLoader(path=data_dir,\n",
    "                              glob=\"*.pdf\",\n",
    "                              loader_cls=PyPDFLoader)\n",
    "     documents = loader.load()\n",
    "     return documents\n",
    "\n",
    "extracted_data = data_extraction(\"../DataSets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to chunk the extracted data\n",
    "def text_chunking(extracted_data):\n",
    "     text_chunker = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=200)\n",
    "     chunks = text_chunker.split_documents(extracted_data)\n",
    "     # return [chunk.page_content for chunk in chunks]\n",
    "     return chunks\n",
    "\n",
    "text_chunks = text_chunking(extracted_data=extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of chunks: 41\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num of chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type of the chunked data\n",
    "type(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for Questions Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_question_generation_pipe = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "                                                     api_key=GEMINI_API_KEY, \n",
    "                                                     temperature=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“„=== Questions from Batch 1 ===\n",
      "\n",
      "AIMessage(content='Okay, here are 5 short answer questions about Text Classification, based on the provided text, with a medium difficulty level, including answers:\\n\\n1.  **Question:** Define text classification and provide a real-world example of its application, different from the email spam filtering example given in the text.\\n    **Answer:** Text classification is the task of assigning one or more categories to a given piece of text from a larger set of possible categories. An example is classifying customer reviews of a product as positive, negative, or neutral to understand customer sentiment.\\n\\n2.  **Question:** Explain the difference between binary, multiclass, and multilabel classification in the context of text classification. Provide an example for each.\\n    **Answer:**\\n    *   **Binary classification:** Categorizing text into one of two classes (e.g., spam or not spam).\\n    *   **Multiclass classification:** Categorizing text into one of several classes, where each text belongs to only one class (e.g., classifying customer reviews as negative, neutral, or positive).\\n    *   **Multilabel classification:** Categorizing text into one or more classes simultaneously (e.g., a news article being classified as \"sports\" and \"soccer\").\\n\\n3.  **Question:** The text mentions that text classification is sometimes referred to as topic classification or document categorization. What NLP task is explicitly differentiated from topic classification in the text, and what does that task involve?\\n    **Answer:** Topic detection is differentiated from topic classification. Topic detection refers to the problem of uncovering or extracting \"topics\" from texts.\\n\\n4.  **Question:** Briefly outline the typical steps involved in building a text classification system, as described in the provided text.\\n    **Answer:** The steps are:\\n    1.  Collect/create a labeled dataset.\\n    2.  Split the dataset into training and test sets and decide on evaluation metrics.\\n    3.  Transform raw text into feature vectors.\\n    4.  Train a classifier using the training set.\\n    5.  Benchmark the model performance on the test set.\\n    6.  Deploy the model and monitor its performance.\\n\\n5.  **Question:** Describe how text classification is used in E-commerce, and explain the evolution of sentiment analysis into \"aspect\"-based sentiment analysis.\\n    **Answer:** In e-commerce, text classification is used to understand and analyze customer perception of products or services through sentiment analysis of customer reviews. Sentiment analysis has evolved into \"aspect\"-based sentiment analysis to understand specific facets of a product or service, rather than just overall sentiment (e.g., identifying that the food was great but the service was bad in a restaurant review).', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-7dd6158a-8800-4d9e-8da0-fc2600196425-0', usage_metadata={'input_tokens': 2806, 'output_tokens': 558, 'total_tokens': 3364, 'input_token_details': {'cache_read': 0}})\n",
      "\n",
      "ðŸ“„=== Questions from Batch 2 ===\n",
      "\n",
      "AIMessage(content='Okay, here are 5 short answer questions about Text Classification, based on the provided text, at a medium difficulty level, with answers:\\n\\n1.  **Question:** According to the text, what are the typical steps in building a text classification system, and which steps are iteratively refined?\\n    **Answer:** The text refers to Figure 4-3, which outlines the typical steps in building a text classification system. Steps 3 through 5 are iterated on to explore different variants of features and classification algorithms, tune hyperparameters, and optimize the model.\\n\\n2.  **Question:** Name at least three evaluation metrics commonly used for text classification, as mentioned in the text.\\n    **Answer:** The text mentions classification accuracy, precision, recall, F1 score, and area under the ROC curve as commonly used evaluation metrics for text classifiers.\\n\\n3.  **Question:** Explain the concept of lexicon-based sentiment analysis as described in the text, and what are its benefits?\\n    **Answer:** Lexicon-based sentiment analysis involves creating dictionaries of positive and negative words and using them to predict the sentiment of a text based on the usage of these words. Benefits include quick deployment of a minimum viable product, better understanding of the problem, and a simple baseline for evaluation.\\n\\n4.  **Question:** According to the text, what is one scenario where it might not be necessary to build your own text classification system?\\n    **Answer:** If the classification task is generic (e.g., identifying a general category of text or sentiment analysis), existing APIs like Google Cloud Natural Language, Microsoft, or Amazon can be used instead of building a custom classifier.\\n\\n5.  **Question:** Describe the \"Economic News Article Tone and Relevance\" dataset used in the text, and what challenge does its imbalance pose?\\n    **Answer:** The dataset consists of 8,000 news articles annotated for relevance to the US economy (yes/no). It is imbalanced, with more non-relevant than relevant articles, which poses the challenge of guarding against learning a bias toward the majority (non-relevant) category.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-ce7ad69f-98ff-4db8-989a-3b69d0e08b83-0', usage_metadata={'input_tokens': 3068, 'output_tokens': 436, 'total_tokens': 3504, 'input_token_details': {'cache_read': 0}})\n",
      "\n",
      "ðŸ“„=== Questions from Batch 3 ===\n",
      "\n",
      "AIMessage(content=\"Here are 5 short answer questions about Text Classification, based on the provided context.\\n\\n1.  **Question:** According to the text, what are the five potential reasons for poor classifier performance?\\n    **Answer:** The five potential reasons are: large, sparse feature vector; class imbalance; need for a better learning algorithm; need for better pre-processing and feature extraction; and need to tune the classifier's parameters and hyperparameters.\\n\\n2.  **Question:** Explain how reducing the number of features in the CountVectorizer can potentially improve text classification performance and why this might be the case.\\n    **Answer:** Reducing the number of features can reduce noise and data sparsity. A large number of features can lead to a sparse feature vector, where most features are zero, affecting the algorithm's ability to learn.\\n\\n3.  **Question:** What are two common approaches to address class imbalance in a text classification task, and which Python library is mentioned as incorporating sampling methods to address this issue?\\n    **Answer:** Two common approaches are oversampling the minority class and undersampling the majority class. The Python library mentioned is Imbalanced-Learn.\\n\\n4.  **Question:** What is the key difference between a generative classifier like Naive Bayes and a discriminative classifier like Logistic Regression, as described in the text?\\n    **Answer:** A generative classifier learns the probability of a text for each class and chooses the one with maximum probability. A discriminative classifier aims to learn the probability distribution over all classes.\\n\\n5.  **Question:** Briefly describe how Support Vector Machines (SVMs) differ from Logistic Regression in their approach to classification, according to the text.\\n    **Answer:** Logistic regression learns weights for individual features and predicts a probability distribution over classes. SVMs aim to find an optimal hyperplane in a higher dimensional space to separate classes with the maximum possible margin and can learn non-linear separations.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-547a63f1-e145-48d1-8970-482842f51a9b-0', usage_metadata={'input_tokens': 2530, 'output_tokens': 394, 'total_tokens': 2924, 'input_token_details': {'cache_read': 0}})\n",
      "\n",
      "ðŸ“„=== Questions from Batch 4 ===\n",
      "\n",
      "AIMessage(content='Okay, here are 5 short answer questions related to text classification, based on the provided context, with a medium difficulty level, and including answers:\\n\\n1.  **Question:** Explain the key advantage of using neural embeddings (like Word2Vec) for text classification compared to traditional BoW or TF-IDF approaches.\\n\\n    **Answer:** Neural embeddings create dense, low-dimensional feature representations, capturing semantic relationships between words, unlike the sparse, high-dimensional representations of BoW/TF-IDF which treat words as independent entities.\\n\\n2.  **Question:** Describe the process of using pre-trained Word2Vec embeddings to create feature vectors for text classification.\\n\\n    **Answer:** The process involves loading a pre-trained Word2Vec model, tokenizing the text, retrieving the word embeddings for each token (if present in the Word2Vec vocabulary), and then averaging these embeddings to create a single feature vector representing the entire text.\\n\\n3.  **Question:** What is the \"out-of-vocabulary\" (OOV) problem in the context of word embeddings, and how does fastText address it?\\n\\n    **Answer:** The OOV problem refers to the situation where a word in the dataset is not present in the pre-trained embedding model\\'s vocabulary. FastText addresses this by using subword-level information, representing words as a sum of character n-gram embeddings, allowing it to generate representations for unseen words.\\n\\n4.  **Question:** According to the text, what is a good rule of thumb to decide whether to train custom word embeddings or use pre-trained embeddings for a specific text classification task?\\n\\n    **Answer:** If the vocabulary overlap between the custom domain and the pre-trained word embeddings is greater than 80%, pre-trained word embeddings tend to give good results. Otherwise, training custom embeddings might be more beneficial.\\n\\n5.  **Question:** Explain how Doc2Vec can be used for text classification, and what makes it different from using word embeddings?\\n\\n    **Answer:** Doc2Vec learns a direct representation for the entire document (sentence/paragraph) rather than individual words. This document embedding can then be used as a feature vector for text classification. Unlike word embeddings, Doc2Vec captures the overall context and meaning of the document directly.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-f159981b-33bf-432b-bbbe-14c6eb62fc0c-0', usage_metadata={'input_tokens': 3524, 'output_tokens': 472, 'total_tokens': 3996, 'input_token_details': {'cache_read': 0}})\n",
      "\n",
      "ðŸ“„=== Questions from Batch 5 ===\n",
      "\n",
      "AIMessage(content='Okay, here are five short answer questions related to text classification using Doc2Vec and Deep Learning, based on the provided text, along with their answers:\\n\\n**Question 1:**\\n\\nWhat is the purpose of the `TaggedDocument` class when training a Doc2Vec model, and what information does it typically contain?\\n\\n**Answer:**\\n\\nThe `TaggedDocument` class is used to format the training data for Doc2Vec. It represents a document as a list of tokens (words) along with a tag, which serves as a unique identifier for that document (e.g., filename or ID).\\n\\n**Question 2:**\\n\\nName three important parameters to consider when training a Doc2Vec model, and briefly explain what each parameter controls.\\n\\n**Answer:**\\n\\n*   **vector_size:** The dimensionality of the learned document embeddings.\\n*   **alpha:** The learning rate for the training process.\\n*   **min_count:** The minimum frequency a word must have to be included in the vocabulary.\\n*   **dm:** Distributed memory, one of the representation learners implemented in Doc2vec (the other is dbow, or distributed bag of words)\\n*   **epochs:** The number of training iterations over the dataset.\\n\\n**Question 3:**\\n\\nHow can the `infer_vector` function in Doc2Vec be used, and why might it be necessary to run it multiple times with aggregation?\\n\\n**Answer:**\\n\\nThe `infer_vector` function is used to generate a vector representation for a new, unseen text document using a pre-trained Doc2Vec model. It\\'s run multiple times (with a specified number of \"steps\") and the resulting vectors are aggregated to obtain a more stable and reliable representation, as the inference process has some inherent randomness.\\n\\n**Question 4:**\\n\\nIn the context of deep learning for text classification, outline the four key steps involved in converting text data into a suitable format for neural network input layers.\\n\\n**Answer:**\\n\\n1.  **Tokenization:** Tokenize the texts and convert them into word index vectors.\\n2.  **Padding:** Pad the text sequences so that all text vectors are of the same length.\\n3.  **Embedding Mapping:** Map every word index to an embedding vector using an embedding matrix (either pre-trained or trained on the corpus).\\n4.  **Input to Neural Network:** Use the output from Step 3 as the input to a neural network architecture.\\n\\n**Question 5:**\\n\\nWhen using CNNs for text classification, how can the convolution and pooling layers be interpreted in terms of feature extraction from the text?\\n\\n**Answer:**\\n\\nIn text classification, CNNs can be seen as learning the most useful bag-of-words/n-grams features from the text, rather than using the entire collection of words/n-grams. The convolution layers extract local features, and the pooling layers downsample these features to retain the most important ones.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-61500957-2bde-4167-b649-236acce5d324-0', usage_metadata={'input_tokens': 3761, 'output_tokens': 609, 'total_tokens': 4370, 'input_token_details': {'cache_read': 0}})\n",
      "\n",
      "ðŸ“„=== Questions from Batch 6 ===\n",
      "\n",
      "AIMessage(content='Okay, here are 5 short answer questions related to Text Classification, based on the provided text, with a medium difficulty level, and including answers:\\n\\n1.  **Question:** According to the text, what are two primary reasons why non-deep learning approaches are still widely used for text classification in industrial settings, despite the advancements in deep learning?\\n    **Answer:** The two primary reasons are a lack of large amounts of task-specific training data required by neural networks, and issues related to computing and deployment costs.\\n\\n2.  **Question:** In the context of using CNNs for text classification, the text mentions a trade-off related to the number of epochs. Explain this trade-off.\\n    **Answer:** Increasing the number of epochs can improve model performance, but it also increases the amount of time it takes to train the model.\\n\\n3.  **Question:** The text highlights a key difference between CNNs and RNNs (specifically LSTMs) in how they handle text data. What is this key difference, and why does it make RNNs well-suited for NLP tasks?\\n    **Answer:** The key difference is that RNNs are specialized in working with sequential data and take into account the context of words in a sentence (words before and after), while CNNs may not inherently consider this sequential nature. This makes RNNs well-suited for NLP tasks because language is sequential in nature.\\n\\n4.  **Question:** Briefly describe how ktrain simplifies the process of using BERT for text classification, as explained in the text.\\n    **Answer:** Ktrain provides a straightforward process for all steps, from obtaining the dataset and the pre-trained BERT model to fine-tuning it for the classification task, using the TensorFlow library Keras.\\n\\n5.  **Question:** Explain how Lime helps in interpreting text classification models.\\n    **Answer:** Lime approximates a black-box classification model with a linear model locally around a given training instance. This linear model is expressed as a weighted sum of its features, making it easier to understand which features contributed most to a particular prediction.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e363bd6d-06fb-4695-9e2a-24b564ff5da5-0', usage_metadata={'input_tokens': 3583, 'output_tokens': 428, 'total_tokens': 4011, 'input_token_details': {'cache_read': 0}})\n",
      "\n",
      "ðŸ“„=== Questions from Batch 7 ===\n",
      "\n",
      "AIMessage(content='Okay, here are 5 short answer questions related to text classification, based on the provided text, with a focus on scenarios with limited data and adapting to new domains:\\n\\n1.  **Question:** Explain the concept of \"bootstrapping\" or \"weak supervision\" in the context of text classification when dealing with no training data. Provide an example of how this could be applied to classifying customer complaints.\\n    **Answer:** Bootstrapping or weak supervision involves using patterns or heuristics to automatically label a small, potentially noisy dataset when no labeled data is initially available. For customer complaints, one could create rules like: \"If a complaint contains words like \\'bill,\\' \\'invoice,\\' or mentions a currency amount, label it as \\'billing-related.\\'\"\\n\\n2.  **Question:** Describe the core principle behind active learning and how it can be used to improve a text classification model when you have limited labeled data.\\n    **Answer:** Active learning focuses on identifying and labeling the most informative data points for training. The model is trained on existing data, used to predict on new data, and then the data points where the model is least confident are sent for human annotation. These newly labeled points are then added to the training set, and the process is repeated.\\n\\n3.  **Question:** What is domain adaptation (or transfer learning) in text classification, and why is it useful when adapting a model to a new product suite or domain?\\n    **Answer:** Domain adaptation is a technique to transfer knowledge learned from a source domain (with abundant data) to a target domain (with less data). It\\'s useful because training a new model from scratch for each new product suite is often impractical due to insufficient training data.\\n\\n4.  **Question:** Briefly outline the steps involved in using a pre-trained language model (like BERT) for domain adaptation in text classification.\\n    **Answer:** 1. Start with a large, pre-trained language model. 2. Fine-tune this model using unlabeled data from the target domain. 3. Train a classifier on the labeled target domain data, using feature representations extracted from the fine-tuned language model.\\n\\n5.  **Question:** In the corporate ticketing case study, what are three potential approaches to building a health issue-related classification system when no past tickets are labeled as health-related?\\n    **Answer:** 1. Use existing APIs or libraries (e.g., Google APIs) and map their medical/health categories to the organization\\'s needs. 2. Adopt public datasets (e.g., 20 Newsgroups) that include medical topics. 3. Utilize weak supervision by creating rules based on keywords (e.g., \"fever,\" \"headache\") to bootstrap a labeled dataset from past tickets.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-71d0d1c5-e594-4818-8c27-aad37776062f-0', usage_metadata={'input_tokens': 2824, 'output_tokens': 572, 'total_tokens': 3396, 'input_token_details': {'cache_read': 0}})\n",
      "\n",
      "ðŸ“„=== Questions from Batch 8 ===\n",
      "\n",
      "AIMessage(content='Okay, here are 5 short answer questions related to Text Classification, based on the provided text, at a medium difficulty level, with answers included:\\n\\n1.  **Question:** According to the text, what are two types of feedback that can be used to improve a text classification system, and how are they defined?\\n    **Answer:** Explicit feedback, which is direct feedback such as a medical counsel stating a ticket is not relevant, and implicit feedback, which is extracted from dependent variables like ticket response times and rates.\\n\\n2.  **Question:** In the context of building a text classifier when training data is limited, what is the initial step recommended in the pipeline described in Figure 4-11?\\n    **Answer:** Start with no labeled data and use either a public API or a model created with a public dataset or weak supervision as the first baseline model.\\n\\n3.  **Question:** The text mentions three reasons for establishing strong baselines when developing text classification algorithms. What are two of these reasons?\\n    **Answer:**\\n    *   It helps us get a better understanding of the problem statement and key challenges.\\n    *   Building a quick MVP helps us get initial feedback from end users and stakeholders.\\n    *   A state-of-the-art research model may give us only a minor improvement compared to the baseline, but it might come with a huge amount of technical debt.\\n\\n4.  **Question:** What are two techniques mentioned in the text for addressing class imbalance in training data for text classification?\\n    **Answer:** Collecting more data, resampling (undersample from majority classes or oversample from minority classes), and weight balancing.\\n\\n5.  **Question:** Besides building the model, what are two other significant aspects of building a text classification system in an industrial setting, as highlighted in the text?\\n    **Answer:** Gathering data, building data pipelines, deployment, testing, and monitoring.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-9144dcb5-d85a-46bc-8158-667ea231931d-0', usage_metadata={'input_tokens': 2703, 'output_tokens': 398, 'total_tokens': 3101, 'input_token_details': {'cache_read': 0}})\n",
      "\n",
      "ðŸ“„=== Questions from Batch 9 ===\n",
      "\n",
      "AIMessage(content=\"Here are 5 short answer questions related to Text Classification, based on the provided context, with a medium difficulty level:\\n\\n1.  **Question:** According to Lundberg and Lee's work (reference [27]), what is a unified approach to interpreting model predictions, and why is it important in the context of text classification?\\n    **Answer:** Lundberg and Lee propose a unified approach using SHAP (SHapley Additive exPlanations) values to explain the output of any machine learning model. This is important in text classification because it allows us to understand which words or phrases in a text contribute most to a particular classification decision, improving model transparency and trust.\\n\\n2.  **Question:** Briefly explain the purpose of LIME (reference [28]) and how it can be applied to understand the predictions of a text classification model.\\n    **Answer:** LIME (Local Interpretable Model-agnostic Explanations) aims to explain the predictions of any machine learning classifier by approximating it locally with an interpretable model. In text classification, LIME can highlight which words in a specific document most influenced the model's prediction for that document.\\n\\n3.  **Question:** What is the main idea behind Snorkel (references [30, 31, 32]) and how does it address the challenge of creating training data for text classification tasks?\\n    **Answer:** Snorkel is a system for programmatically building and managing training data using weak supervision. It addresses the challenge of creating labeled data for text classification by allowing users to define labeling functions (e.g., using patterns or heuristics) that automatically generate noisy labels, which are then de-noised and used to train a model.\\n\\n4.  **Question:** Based on the context, what is the purpose of Prodigy (reference [33]) in the context of text classification and data annotation?\\n    **Answer:** Prodigy is a tool designed for efficient data annotation. In text classification, it allows human annotators to quickly label text data, improving the quality and speed of training data creation compared to manual methods.\\n\\n5.  **Question:** According to Caruana et al. (reference [36]), what is ensemble selection and how can it improve text classification performance?\\n    **Answer:** Ensemble selection involves choosing a subset of models from a larger library of trained models to form an ensemble. This can improve text classification performance by selecting models that complement each other and generalize well to unseen data, potentially outperforming a single best model or a full ensemble.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-7f57c6c5-2d37-4beb-87d3-84d1790b63e4-0', usage_metadata={'input_tokens': 717, 'output_tokens': 523, 'total_tokens': 1240, 'input_token_details': {'cache_read': 0}})\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"You are a knowledgeable and professional AI assistant that specializes in generating high-quality exam questions.\n",
    "\n",
    "                         Your role is to:\n",
    "\n",
    "                         1. Generate clear, accurate, and well-structured exam questions based on the provided context and requirements.\n",
    "                         2. Ensure that questions are relevant to the given subject, topic, and difficulty level.\n",
    "                         3. Vary the question types if requested (e.g., multiple choice, true/false, short answer, essay).\n",
    "                         4. Provide correct answers or model answers if specified.\n",
    "                         5. Follow academic standards and avoid overly simplistic or overly complex wording unless specified.\n",
    "                         6. If context is not enough to generate meaningful questions, acknowledge the limitation and ask for more detail.\n",
    "\n",
    "                         Remember:\n",
    "                         - Do not make up unrelated information.\n",
    "                         - Stick closely to the topic and subject area.\n",
    "                         - Maintain clarity and educational value in all questions.\n",
    "                         - Avoid repetition unless explicitly instructed.\n",
    "                         - Ensure consistency with the question format and style.\n",
    "\n",
    "                         Context:\n",
    "                         {context}\n",
    "\n",
    "                         Instructions:\n",
    "                         - Subject: {subject}\n",
    "                         - Topic: {topic}\n",
    "                         - Number of Questions: {num_questions}\n",
    "                         - Difficulty Level: {difficulty_level}\n",
    "                         - Question Type(s): {question_types}\n",
    "                         - Include Answers: {include_answers}\n",
    "\n",
    "                         Now, generate the questions based on the above.\n",
    "                         \"\"\"\n",
    "\n",
    "# Create the Langchain PromptTemplate\n",
    "exam_prompt = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"context\",\n",
    "        \"subject\",\n",
    "        \"topic\",\n",
    "        \"num_questions\",\n",
    "        \"difficulty_level\",\n",
    "        \"question_types\",\n",
    "        \"include_answers\"\n",
    "    ],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# Batch size for processing\n",
    "batch_size = 5\n",
    "num_chunks = len(text_chunks) \n",
    "\n",
    "dic_Exam_ques = {}\n",
    "\n",
    "for i in range(0, num_chunks, batch_size):\n",
    "    batch = text_chunks[i:i+batch_size]\n",
    "    \n",
    "   \n",
    "    batch_context = \"\\n\\n\".join([chunk.page_content for chunk in batch])\n",
    "    \n",
    "\n",
    "    filled_prompt = exam_prompt.format(\n",
    "        context=batch_context,\n",
    "        subject=\"Natural Language Processing\",\n",
    "        topic=\"Text Classification\",\n",
    "        num_questions=\"5\",  \n",
    "        difficulty_level=\"Medium\",\n",
    "        question_types=\"short answer\",\n",
    "        include_answers=\"Yes\"\n",
    "    )\n",
    "    print(f\"\\nðŸ“„=== Questions from Batch {i//batch_size + 1} ===\\n\")\n",
    "    response = llm_question_generation_pipe.invoke(filled_prompt)\n",
    "    pprint(response)\n",
    "    response = response.content\n",
    "    response = re.sub(r'\\s+', ' ', response).strip()\n",
    "    question = response[response.index(\"Question\"): response.index(\"Answer\")]\n",
    "    answer = response[response.index(\"Answer\"):]\n",
    "    dic_Exam_ques[question] = answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question:** Define text classification and provide a real-world example of its application, different from the email spam filtering example given in the text. **': 'Answer:** Text classification is the task of assigning one or more categories to a given piece of text from a larger set of possible categories. An example is classifying customer reviews of a product as positive, negative, or neutral to understand customer sentiment. 2. **Question:** Explain the difference between binary, multiclass, and multilabel classification in the context of text classification. Provide an example for each. **Answer:** * **Binary classification:** Categorizing text into one of two classes (e.g., spam or not spam). * **Multiclass classification:** Categorizing text into one of several classes, where each text belongs to only one class (e.g., classifying customer reviews as negative, neutral, or positive). * **Multilabel classification:** Categorizing text into one or more classes simultaneously (e.g., a news article being classified as \"sports\" and \"soccer\"). 3. **Question:** The text mentions that text classification is sometimes referred to as topic classification or document categorization. What NLP task is explicitly differentiated from topic classification in the text, and what does that task involve? **Answer:** Topic detection is differentiated from topic classification. Topic detection refers to the problem of uncovering or extracting \"topics\" from texts. 4. **Question:** Briefly outline the typical steps involved in building a text classification system, as described in the provided text. **Answer:** The steps are: 1. Collect/create a labeled dataset. 2. Split the dataset into training and test sets and decide on evaluation metrics. 3. Transform raw text into feature vectors. 4. Train a classifier using the training set. 5. Benchmark the model performance on the test set. 6. Deploy the model and monitor its performance. 5. **Question:** Describe how text classification is used in E-commerce, and explain the evolution of sentiment analysis into \"aspect\"-based sentiment analysis. **Answer:** In e-commerce, text classification is used to understand and analyze customer perception of products or services through sentiment analysis of customer reviews. Sentiment analysis has evolved into \"aspect\"-based sentiment analysis to understand specific facets of a product or service, rather than just overall sentiment (e.g., identifying that the food was great but the service was bad in a restaurant review).',\n",
       " 'Question:** According to the text, what are the typical steps in building a text classification system, and which steps are iteratively refined? **': 'Answer:** The text refers to Figure 4-3, which outlines the typical steps in building a text classification system. Steps 3 through 5 are iterated on to explore different variants of features and classification algorithms, tune hyperparameters, and optimize the model. 2. **Question:** Name at least three evaluation metrics commonly used for text classification, as mentioned in the text. **Answer:** The text mentions classification accuracy, precision, recall, F1 score, and area under the ROC curve as commonly used evaluation metrics for text classifiers. 3. **Question:** Explain the concept of lexicon-based sentiment analysis as described in the text, and what are its benefits? **Answer:** Lexicon-based sentiment analysis involves creating dictionaries of positive and negative words and using them to predict the sentiment of a text based on the usage of these words. Benefits include quick deployment of a minimum viable product, better understanding of the problem, and a simple baseline for evaluation. 4. **Question:** According to the text, what is one scenario where it might not be necessary to build your own text classification system? **Answer:** If the classification task is generic (e.g., identifying a general category of text or sentiment analysis), existing APIs like Google Cloud Natural Language, Microsoft, or Amazon can be used instead of building a custom classifier. 5. **Question:** Describe the \"Economic News Article Tone and Relevance\" dataset used in the text, and what challenge does its imbalance pose? **Answer:** The dataset consists of 8,000 news articles annotated for relevance to the US economy (yes/no). It is imbalanced, with more non-relevant than relevant articles, which poses the challenge of guarding against learning a bias toward the majority (non-relevant) category.',\n",
       " 'Question:** According to the text, what are the five potential reasons for poor classifier performance? **': \"Answer:** The five potential reasons are: large, sparse feature vector; class imbalance; need for a better learning algorithm; need for better pre-processing and feature extraction; and need to tune the classifier's parameters and hyperparameters. 2. **Question:** Explain how reducing the number of features in the CountVectorizer can potentially improve text classification performance and why this might be the case. **Answer:** Reducing the number of features can reduce noise and data sparsity. A large number of features can lead to a sparse feature vector, where most features are zero, affecting the algorithm's ability to learn. 3. **Question:** What are two common approaches to address class imbalance in a text classification task, and which Python library is mentioned as incorporating sampling methods to address this issue? **Answer:** Two common approaches are oversampling the minority class and undersampling the majority class. The Python library mentioned is Imbalanced-Learn. 4. **Question:** What is the key difference between a generative classifier like Naive Bayes and a discriminative classifier like Logistic Regression, as described in the text? **Answer:** A generative classifier learns the probability of a text for each class and chooses the one with maximum probability. A discriminative classifier aims to learn the probability distribution over all classes. 5. **Question:** Briefly describe how Support Vector Machines (SVMs) differ from Logistic Regression in their approach to classification, according to the text. **Answer:** Logistic regression learns weights for individual features and predicts a probability distribution over classes. SVMs aim to find an optimal hyperplane in a higher dimensional space to separate classes with the maximum possible margin and can learn non-linear separations.\",\n",
       " 'Question:** Explain the key advantage of using neural embeddings (like Word2Vec) for text classification compared to traditional BoW or TF-IDF approaches. **': 'Answer:** Neural embeddings create dense, low-dimensional feature representations, capturing semantic relationships between words, unlike the sparse, high-dimensional representations of BoW/TF-IDF which treat words as independent entities. 2. **Question:** Describe the process of using pre-trained Word2Vec embeddings to create feature vectors for text classification. **Answer:** The process involves loading a pre-trained Word2Vec model, tokenizing the text, retrieving the word embeddings for each token (if present in the Word2Vec vocabulary), and then averaging these embeddings to create a single feature vector representing the entire text. 3. **Question:** What is the \"out-of-vocabulary\" (OOV) problem in the context of word embeddings, and how does fastText address it? **Answer:** The OOV problem refers to the situation where a word in the dataset is not present in the pre-trained embedding model\\'s vocabulary. FastText addresses this by using subword-level information, representing words as a sum of character n-gram embeddings, allowing it to generate representations for unseen words. 4. **Question:** According to the text, what is a good rule of thumb to decide whether to train custom word embeddings or use pre-trained embeddings for a specific text classification task? **Answer:** If the vocabulary overlap between the custom domain and the pre-trained word embeddings is greater than 80%, pre-trained word embeddings tend to give good results. Otherwise, training custom embeddings might be more beneficial. 5. **Question:** Explain how Doc2Vec can be used for text classification, and what makes it different from using word embeddings? **Answer:** Doc2Vec learns a direct representation for the entire document (sentence/paragraph) rather than individual words. This document embedding can then be used as a feature vector for text classification. Unlike word embeddings, Doc2Vec captures the overall context and meaning of the document directly.',\n",
       " 'Question 1:** What is the purpose of the `TaggedDocument` class when training a Doc2Vec model, and what information does it typically contain? **': 'Answer:** The `TaggedDocument` class is used to format the training data for Doc2Vec. It represents a document as a list of tokens (words) along with a tag, which serves as a unique identifier for that document (e.g., filename or ID). **Question 2:** Name three important parameters to consider when training a Doc2Vec model, and briefly explain what each parameter controls. **Answer:** * **vector_size:** The dimensionality of the learned document embeddings. * **alpha:** The learning rate for the training process. * **min_count:** The minimum frequency a word must have to be included in the vocabulary. * **dm:** Distributed memory, one of the representation learners implemented in Doc2vec (the other is dbow, or distributed bag of words) * **epochs:** The number of training iterations over the dataset. **Question 3:** How can the `infer_vector` function in Doc2Vec be used, and why might it be necessary to run it multiple times with aggregation? **Answer:** The `infer_vector` function is used to generate a vector representation for a new, unseen text document using a pre-trained Doc2Vec model. It\\'s run multiple times (with a specified number of \"steps\") and the resulting vectors are aggregated to obtain a more stable and reliable representation, as the inference process has some inherent randomness. **Question 4:** In the context of deep learning for text classification, outline the four key steps involved in converting text data into a suitable format for neural network input layers. **Answer:** 1. **Tokenization:** Tokenize the texts and convert them into word index vectors. 2. **Padding:** Pad the text sequences so that all text vectors are of the same length. 3. **Embedding Mapping:** Map every word index to an embedding vector using an embedding matrix (either pre-trained or trained on the corpus). 4. **Input to Neural Network:** Use the output from Step 3 as the input to a neural network architecture. **Question 5:** When using CNNs for text classification, how can the convolution and pooling layers be interpreted in terms of feature extraction from the text? **Answer:** In text classification, CNNs can be seen as learning the most useful bag-of-words/n-grams features from the text, rather than using the entire collection of words/n-grams. The convolution layers extract local features, and the pooling layers downsample these features to retain the most important ones.',\n",
       " 'Question:** According to the text, what are two primary reasons why non-deep learning approaches are still widely used for text classification in industrial settings, despite the advancements in deep learning? **': 'Answer:** The two primary reasons are a lack of large amounts of task-specific training data required by neural networks, and issues related to computing and deployment costs. 2. **Question:** In the context of using CNNs for text classification, the text mentions a trade-off related to the number of epochs. Explain this trade-off. **Answer:** Increasing the number of epochs can improve model performance, but it also increases the amount of time it takes to train the model. 3. **Question:** The text highlights a key difference between CNNs and RNNs (specifically LSTMs) in how they handle text data. What is this key difference, and why does it make RNNs well-suited for NLP tasks? **Answer:** The key difference is that RNNs are specialized in working with sequential data and take into account the context of words in a sentence (words before and after), while CNNs may not inherently consider this sequential nature. This makes RNNs well-suited for NLP tasks because language is sequential in nature. 4. **Question:** Briefly describe how ktrain simplifies the process of using BERT for text classification, as explained in the text. **Answer:** Ktrain provides a straightforward process for all steps, from obtaining the dataset and the pre-trained BERT model to fine-tuning it for the classification task, using the TensorFlow library Keras. 5. **Question:** Explain how Lime helps in interpreting text classification models. **Answer:** Lime approximates a black-box classification model with a linear model locally around a given training instance. This linear model is expressed as a weighted sum of its features, making it easier to understand which features contributed most to a particular prediction.',\n",
       " 'Question:** Explain the concept of \"bootstrapping\" or \"weak supervision\" in the context of text classification when dealing with no training data. Provide an example of how this could be applied to classifying customer complaints. **': 'Answer:** Bootstrapping or weak supervision involves using patterns or heuristics to automatically label a small, potentially noisy dataset when no labeled data is initially available. For customer complaints, one could create rules like: \"If a complaint contains words like \\'bill,\\' \\'invoice,\\' or mentions a currency amount, label it as \\'billing-related.\\'\" 2. **Question:** Describe the core principle behind active learning and how it can be used to improve a text classification model when you have limited labeled data. **Answer:** Active learning focuses on identifying and labeling the most informative data points for training. The model is trained on existing data, used to predict on new data, and then the data points where the model is least confident are sent for human annotation. These newly labeled points are then added to the training set, and the process is repeated. 3. **Question:** What is domain adaptation (or transfer learning) in text classification, and why is it useful when adapting a model to a new product suite or domain? **Answer:** Domain adaptation is a technique to transfer knowledge learned from a source domain (with abundant data) to a target domain (with less data). It\\'s useful because training a new model from scratch for each new product suite is often impractical due to insufficient training data. 4. **Question:** Briefly outline the steps involved in using a pre-trained language model (like BERT) for domain adaptation in text classification. **Answer:** 1. Start with a large, pre-trained language model. 2. Fine-tune this model using unlabeled data from the target domain. 3. Train a classifier on the labeled target domain data, using feature representations extracted from the fine-tuned language model. 5. **Question:** In the corporate ticketing case study, what are three potential approaches to building a health issue-related classification system when no past tickets are labeled as health-related? **Answer:** 1. Use existing APIs or libraries (e.g., Google APIs) and map their medical/health categories to the organization\\'s needs. 2. Adopt public datasets (e.g., 20 Newsgroups) that include medical topics. 3. Utilize weak supervision by creating rules based on keywords (e.g., \"fever,\" \"headache\") to bootstrap a labeled dataset from past tickets.',\n",
       " 'Question:** According to the text, what are two types of feedback that can be used to improve a text classification system, and how are they defined? **': 'Answer:** Explicit feedback, which is direct feedback such as a medical counsel stating a ticket is not relevant, and implicit feedback, which is extracted from dependent variables like ticket response times and rates. 2. **Question:** In the context of building a text classifier when training data is limited, what is the initial step recommended in the pipeline described in Figure 4-11? **Answer:** Start with no labeled data and use either a public API or a model created with a public dataset or weak supervision as the first baseline model. 3. **Question:** The text mentions three reasons for establishing strong baselines when developing text classification algorithms. What are two of these reasons? **Answer:** * It helps us get a better understanding of the problem statement and key challenges. * Building a quick MVP helps us get initial feedback from end users and stakeholders. * A state-of-the-art research model may give us only a minor improvement compared to the baseline, but it might come with a huge amount of technical debt. 4. **Question:** What are two techniques mentioned in the text for addressing class imbalance in training data for text classification? **Answer:** Collecting more data, resampling (undersample from majority classes or oversample from minority classes), and weight balancing. 5. **Question:** Besides building the model, what are two other significant aspects of building a text classification system in an industrial setting, as highlighted in the text? **Answer:** Gathering data, building data pipelines, deployment, testing, and monitoring.',\n",
       " \"Question:** According to Lundberg and Lee's work (reference [27]), what is a unified approach to interpreting model predictions, and why is it important in the context of text classification? **\": \"Answer:** Lundberg and Lee propose a unified approach using SHAP (SHapley Additive exPlanations) values to explain the output of any machine learning model. This is important in text classification because it allows us to understand which words or phrases in a text contribute most to a particular classification decision, improving model transparency and trust. 2. **Question:** Briefly explain the purpose of LIME (reference [28]) and how it can be applied to understand the predictions of a text classification model. **Answer:** LIME (Local Interpretable Model-agnostic Explanations) aims to explain the predictions of any machine learning classifier by approximating it locally with an interpretable model. In text classification, LIME can highlight which words in a specific document most influenced the model's prediction for that document. 3. **Question:** What is the main idea behind Snorkel (references [30, 31, 32]) and how does it address the challenge of creating training data for text classification tasks? **Answer:** Snorkel is a system for programmatically building and managing training data using weak supervision. It addresses the challenge of creating labeled data for text classification by allowing users to define labeling functions (e.g., using patterns or heuristics) that automatically generate noisy labels, which are then de-noised and used to train a model. 4. **Question:** Based on the context, what is the purpose of Prodigy (reference [33]) in the context of text classification and data annotation? **Answer:** Prodigy is a tool designed for efficient data annotation. In text classification, it allows human annotators to quickly label text data, improving the quality and speed of training data creation compared to manual methods. 5. **Question:** According to Caruana et al. (reference [36]), what is ensemble selection and how can it improve text classification performance? **Answer:** Ensemble selection involves choosing a subset of models from a larger library of trained models to form an ensemble. This can improve text classification performance by selecting models that complement each other and generalize well to unseen data, potentially outperforming a single best model or a full ensemble.\"}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_Exam_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
